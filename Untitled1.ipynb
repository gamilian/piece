{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cbb406f7-4910-4de5-bf60-12956d5026dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.cuda.amp import autocast\n",
    "from torchvision.ops import RoIAlign\n",
    "\n",
    "def get_roi_align_layer(output_size=(4, 4), spatial_scale=0.0625, sampling_ratio=0):\n",
    "    # 创建 ROI Align 层\n",
    "    roi_align = RoIAlign(output_size, spatial_scale, sampling_ratio)\n",
    "    return roi_align\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    return img\n",
    "    \n",
    "class JigsawViT(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_cfg_file, num_labels=2):\n",
    "        super(JigsawViT, self).__init__()\n",
    "        # pretrained_cfg = timm.models.create_model(\n",
    "        #     'crossvit_base_240.in1k').default_cfg\n",
    "        # pretrained_cfg['file'] = pretrained_cfg_file\n",
    "        # self.model = timm.models.create_model(\n",
    "        #     'crossvit_base_240.in1k', pretrained=True, num_classes=num_labels, pretrained_cfg=pretrained_cfg)\n",
    "        pretrained_cfg = timm.models.create_model(\n",
    "            'pit_s_distilled_224.in1k').default_cfg\n",
    "        print(pretrained_cfg)\n",
    "        pretrained_cfg['file'] = pretrained_cfg_file\n",
    "        self.backbone = timm.models.create_model(\n",
    "            'pit_s_distilled_224.in1k', pretrained=True, features_only=True, pretrained_cfg=pretrained_cfg)\n",
    "        self.roi_align = get_roi_align_layer()\n",
    "        self.fc1 = nn.Linear(9216, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 512)\n",
    "        self.fc3 = nn.Linear(512, num_labels)\n",
    "        \n",
    "        \n",
    "    # @autocast()\n",
    "    def forward(self, x, roi):\n",
    "        features = self.backbone(x)[-1]\n",
    "        aligned_features = self.roi_align(features, rois)\n",
    "        x = aligned_features.view(aligned_features.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        output = self.fc3(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "322e7e8e-e420-4b87-8b8d-f3dfc72977f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': '', 'hf_hub_id': 'timm/pit_s_distilled_224.in1k', 'architecture': 'pit_s_distilled_224', 'tag': 'in1k', 'custom_load': False, 'input_size': (3, 224, 224), 'fixed_input_size': True, 'interpolation': 'bicubic', 'crop_pct': 0.9, 'crop_mode': 'center', 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225), 'num_classes': 1000, 'pool_size': None, 'first_conv': 'patch_embed.conv', 'classifier': ('head', 'head_dist')}\n",
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "image = preprocess_image(\"dataset/MIT_ex/fragment_0001.png\")\n",
    "pretrained_cfg_file='/data/csl/code/piece/models/pit_s-distilled_224/model.safetensors'\n",
    "jit = JigsawViT(pretrained_cfg_file, 2)\n",
    "rois = torch.tensor([[0, 60, 30, 180, 150]], dtype=torch.float32)\n",
    "output = jit(image, rois)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2a637766-3a06-4e01-b1c5-94defa7fd7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 576, 7, 7])\n",
      "torch.Size([1, 9216])\n",
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import timm\n",
    "from torchvision.ops import RoIAlign\n",
    "\n",
    "\n",
    "\n",
    "def load_pit_backbone():\n",
    "    # 加载预训练的 PiT 模型\n",
    "    pretrained_cfg = timm.models.create_model(\n",
    "            'pit_s_distilled_224.in1k').default_cfg\n",
    "    pretrained_cfg['file'] = pretrained_cfg_file\n",
    "    model = timm.models.create_model(\n",
    "        'pit_s_distilled_224.in1k', pretrained=True, features_only=True, pretrained_cfg=pretrained_cfg)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_roi_align_layer(output_size=(4, 4), spatial_scale=0.0625, sampling_ratio=0):\n",
    "    # 创建 ROI Align 层\n",
    "    roi_align = RoIAlign(output_size, spatial_scale, sampling_ratio)\n",
    "    return roi_align\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    return img\n",
    "def extract_features(backbone, image):\n",
    "    # 通过 Backbone 提取特征\n",
    "    features = backbone(image)[-1]\n",
    "    return features\n",
    "def extract_roi_features(roi_align, features, rois):\n",
    "    # 使用 ROI Align 提取特征\n",
    "    aligned_features = roi_align(features, rois)\n",
    "    return aligned_features\n",
    "\n",
    "backbone = load_pit_backbone()\n",
    "roi_align = get_roi_align_layer()\n",
    "\n",
    "image = preprocess_image(\"dataset/MIT_ex/fragment_0001.png\")\n",
    "features = extract_features(backbone, image)\n",
    "print(features.shape)\n",
    "\n",
    "# 假设的ROI\n",
    "rois = torch.tensor([[0, 60, 30, 180, 150]], dtype=torch.float32)\n",
    "\n",
    "aligned_features = extract_roi_features(roi_align, features, rois)\n",
    "aligned_features = aligned_features.view(aligned_features.size(0), -1)\n",
    "print(aligned_features.shape)\n",
    "fc = nn.Linear(aligned_features.shape[1], 2)\n",
    "f = fc(aligned_features)\n",
    "print(f.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7ddb4b75-2b5f-46f5-923c-9cc55b02f97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_942232/1404160061.py:233: UserWarning: Overwriting pit_b in registry with __main__.pit_b. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def pit_b(pretrained, **kwargs):\n",
      "/tmp/ipykernel_942232/1404160061.py:251: UserWarning: Overwriting pit_s in registry with __main__.pit_s. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def pit_s(pretrained, **kwargs):\n",
      "/tmp/ipykernel_942232/1404160061.py:270: UserWarning: Overwriting pit_xs in registry with __main__.pit_xs. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def pit_xs(pretrained, **kwargs):\n",
      "/tmp/ipykernel_942232/1404160061.py:288: UserWarning: Overwriting pit_ti in registry with __main__.pit_ti. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def pit_ti(pretrained, **kwargs):\n",
      "/tmp/ipykernel_942232/1404160061.py:307: UserWarning: Overwriting pit_b_distilled in registry with __main__.pit_b_distilled. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def pit_b_distilled(pretrained, **kwargs):\n",
      "/tmp/ipykernel_942232/1404160061.py:326: UserWarning: Overwriting pit_s_distilled in registry with __main__.pit_s_distilled. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def pit_s_distilled(pretrained, **kwargs):\n",
      "/tmp/ipykernel_942232/1404160061.py:345: UserWarning: Overwriting pit_xs_distilled in registry with __main__.pit_xs_distilled. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def pit_xs_distilled(pretrained, **kwargs):\n",
      "/tmp/ipykernel_942232/1404160061.py:364: UserWarning: Overwriting pit_ti_distilled in registry with __main__.pit_ti_distilled. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def pit_ti_distilled(pretrained, **kwargs):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from torch.cuda.amp import autocast\n",
    "from torchvision.ops import RoIAlign\n",
    "\n",
    "def get_roi_align_layer(output_size=(4, 4), spatial_scale=0.0625, sampling_ratio=0):\n",
    "    # 创建 ROI Align 层\n",
    "    roi_align = RoIAlign(output_size, spatial_scale, sampling_ratio)\n",
    "    return roi_align\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    return img\n",
    "\n",
    "\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "from functools import partial\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.models.vision_transformer import Block as transformer_block\n",
    "from timm.models.registry import register_model\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, base_dim, depth, heads, mlp_ratio,\n",
    "                 drop_rate=.0, attn_drop_rate=.0, drop_path_prob=None):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        embed_dim = base_dim * heads\n",
    "\n",
    "        if drop_path_prob is None:\n",
    "            drop_path_prob = [0.0 for _ in range(depth)]\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            transformer_block(\n",
    "                dim=embed_dim,\n",
    "                num_heads=heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=True,\n",
    "                proj_drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=drop_path_prob[i],\n",
    "                norm_layer=partial(nn.LayerNorm, eps=1e-6)\n",
    "            )\n",
    "            for i in range(depth)])\n",
    "\n",
    "    def forward(self, x, cls_tokens):\n",
    "        h, w = x.shape[2:4]\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "        token_length = cls_tokens.shape[1]\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        cls_tokens = x[:, :token_length]\n",
    "        x = x[:, token_length:]\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "\n",
    "        return x, cls_tokens\n",
    "\n",
    "\n",
    "class conv_head_pooling(nn.Module):\n",
    "    def __init__(self, in_feature, out_feature, stride,\n",
    "                 padding_mode='zeros'):\n",
    "        super(conv_head_pooling, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_feature, out_feature, kernel_size=stride + 1,\n",
    "                              padding=stride // 2, stride=stride,\n",
    "                              padding_mode=padding_mode, groups=in_feature)\n",
    "        self.fc = nn.Linear(in_feature, out_feature)\n",
    "\n",
    "    def forward(self, x, cls_token):\n",
    "\n",
    "        x = self.conv(x)\n",
    "        cls_token = self.fc(cls_token)\n",
    "\n",
    "        return x, cls_token\n",
    "\n",
    "\n",
    "class conv_embedding(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, patch_size,\n",
    "                 stride, padding):\n",
    "        super(conv_embedding, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=patch_size,\n",
    "                              stride=stride, padding=padding, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PoolingTransformer(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, stride, base_dims, depth, heads,\n",
    "                 mlp_ratio, num_classes=1000, in_chans=3,\n",
    "                 attn_drop_rate=.0, drop_rate=.0, drop_path_rate=.0):\n",
    "        super(PoolingTransformer, self).__init__()\n",
    "\n",
    "        total_block = sum(depth)\n",
    "        padding = 0\n",
    "        block_idx = 0\n",
    "\n",
    "        width = math.floor(\n",
    "            (image_size + 2 * padding - patch_size) / stride + 1)\n",
    "\n",
    "        self.base_dims = base_dims\n",
    "        self.heads = heads\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.randn(1, base_dims[0] * heads[0], width, width),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        self.patch_embed = conv_embedding(in_chans, base_dims[0] * heads[0],\n",
    "                                          patch_size, stride, padding)\n",
    "\n",
    "        self.cls_token = nn.Parameter(\n",
    "            torch.randn(1, 1, base_dims[0] * heads[0]),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        self.transformers = nn.ModuleList([])\n",
    "        self.pools = nn.ModuleList([])\n",
    "\n",
    "        for stage in range(len(depth)):\n",
    "            drop_path_prob = [drop_path_rate * i / total_block\n",
    "                              for i in range(block_idx, block_idx + depth[stage])]\n",
    "            block_idx += depth[stage]\n",
    "\n",
    "            self.transformers.append(\n",
    "                Transformer(base_dims[stage], depth[stage], heads[stage],\n",
    "                            mlp_ratio,\n",
    "                            drop_rate, attn_drop_rate, drop_path_prob)\n",
    "            )\n",
    "            if stage < len(heads) - 1:\n",
    "                self.pools.append(\n",
    "                    conv_head_pooling(base_dims[stage] * heads[stage],\n",
    "                                      base_dims[stage + 1] * heads[stage + 1],\n",
    "                                      stride=2\n",
    "                                      )\n",
    "                )\n",
    "\n",
    "        self.norm = nn.LayerNorm(base_dims[-1] * heads[-1], eps=1e-6)\n",
    "        self.embed_dim = base_dims[-1] * heads[-1]\n",
    "\n",
    "        # Classifier head\n",
    "        if num_classes > 0:\n",
    "            self.head = nn.Linear(base_dims[-1] * heads[-1], num_classes)\n",
    "        else:\n",
    "            self.head = nn.Identity()\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        if num_classes > 0:\n",
    "            self.head = nn.Linear(self.embed_dim, num_classes)\n",
    "        else:\n",
    "            self.head = nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        pos_embed = self.pos_embed\n",
    "        x = self.pos_drop(x + pos_embed)\n",
    "        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "\n",
    "        for stage in range(len(self.pools)):\n",
    "            x, cls_tokens = self.transformers[stage](x, cls_tokens)\n",
    "            x, cls_tokens = self.pools[stage](x, cls_tokens)\n",
    "        x, cls_tokens = self.transformers[-1](x, cls_tokens)\n",
    "\n",
    "        cls_tokens = self.norm(cls_tokens)\n",
    "\n",
    "        return cls_tokens\n",
    "\n",
    "    def forward(self, x):\n",
    "        cls_token = self.forward_features(x)\n",
    "        cls_token = self.head(cls_token[:, 0])\n",
    "        return cls_token\n",
    "\n",
    "\n",
    "class DistilledPoolingTransformer(PoolingTransformer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.cls_token = nn.Parameter(\n",
    "            torch.randn(1, 2, self.base_dims[0] * self.heads[0]),\n",
    "            requires_grad=True)\n",
    "        if self.num_classes > 0:\n",
    "            self.head_dist = nn.Linear(self.base_dims[-1] * self.heads[-1],\n",
    "                                       self.num_classes)\n",
    "        else:\n",
    "            self.head_dist = nn.Identity()\n",
    "\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.head_dist.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cls_token = self.forward_features(x)\n",
    "        x_cls = self.head(cls_token[:, 0])\n",
    "        x_dist = self.head_dist(cls_token[:, 1])\n",
    "        if self.training:\n",
    "            return x_cls, x_dist\n",
    "        else:\n",
    "            return (x_cls + x_dist) / 2\n",
    "\n",
    "@register_model\n",
    "def pit_b(pretrained, **kwargs):\n",
    "    model = PoolingTransformer(\n",
    "        image_size=224,\n",
    "        patch_size=14,\n",
    "        stride=7,\n",
    "        base_dims=[64, 64, 64],\n",
    "        depth=[3, 6, 4],\n",
    "        heads=[4, 8, 16],\n",
    "        mlp_ratio=4,\n",
    "        **kwargs\n",
    "    )\n",
    "    if pretrained:\n",
    "        state_dict = \\\n",
    "        torch.load('weights/pit_b_820.pth', map_location='cpu')\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def pit_s(pretrained, **kwargs):\n",
    "    model = PoolingTransformer(\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "        stride=8,\n",
    "        base_dims=[48, 48, 48],\n",
    "        depth=[2, 6, 4],\n",
    "        heads=[3, 6, 12],\n",
    "        mlp_ratio=4,\n",
    "        **kwargs\n",
    "    )\n",
    "    if pretrained:\n",
    "        state_dict = \\\n",
    "        torch.load('weights/pit_s_809.pth', map_location='cpu')\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def pit_xs(pretrained, **kwargs):\n",
    "    model = PoolingTransformer(\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "        stride=8,\n",
    "        base_dims=[48, 48, 48],\n",
    "        depth=[2, 6, 4],\n",
    "        heads=[2, 4, 8],\n",
    "        mlp_ratio=4,\n",
    "        **kwargs\n",
    "    )\n",
    "    if pretrained:\n",
    "        state_dict = \\\n",
    "        torch.load('weights/pit_xs_781.pth', map_location='cpu')\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def pit_ti(pretrained, **kwargs):\n",
    "    model = PoolingTransformer(\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "        stride=8,\n",
    "        base_dims=[32, 32, 32],\n",
    "        depth=[2, 6, 4],\n",
    "        heads=[2, 4, 8],\n",
    "        mlp_ratio=4,\n",
    "        **kwargs\n",
    "    )\n",
    "    if pretrained:\n",
    "        state_dict = \\\n",
    "        torch.load('weights/pit_ti_730.pth', map_location='cpu')\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def pit_b_distilled(pretrained, **kwargs):\n",
    "    model = DistilledPoolingTransformer(\n",
    "        image_size=224,\n",
    "        patch_size=14,\n",
    "        stride=7,\n",
    "        base_dims=[64, 64, 64],\n",
    "        depth=[3, 6, 4],\n",
    "        heads=[4, 8, 16],\n",
    "        mlp_ratio=4,\n",
    "        **kwargs\n",
    "    )\n",
    "    if pretrained:\n",
    "        state_dict = \\\n",
    "        torch.load('weights/pit_b_distill_840.pth', map_location='cpu')\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def pit_s_distilled(pretrained, **kwargs):\n",
    "    model = DistilledPoolingTransformer(\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "        stride=8,\n",
    "        base_dims=[48, 48, 48],\n",
    "        depth=[2, 6, 4],\n",
    "        heads=[3, 6, 12],\n",
    "        mlp_ratio=4,\n",
    "        **kwargs\n",
    "    )\n",
    "    if pretrained:\n",
    "        state_dict = \\\n",
    "        torch.load('weights/pit_s_distill_819.pth', map_location='cpu')\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def pit_xs_distilled(pretrained, **kwargs):\n",
    "    model = DistilledPoolingTransformer(\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "        stride=8,\n",
    "        base_dims=[48, 48, 48],\n",
    "        depth=[2, 6, 4],\n",
    "        heads=[2, 4, 8],\n",
    "        mlp_ratio=4,\n",
    "        **kwargs\n",
    "    )\n",
    "    if pretrained:\n",
    "        state_dict = \\\n",
    "        torch.load('weights/pit_xs_distill_791.pth', map_location='cpu')\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def pit_ti_distilled(pretrained, **kwargs):\n",
    "    model = DistilledPoolingTransformer(\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "        stride=8,\n",
    "        base_dims=[32, 32, 32],\n",
    "        depth=[2, 6, 4],\n",
    "        heads=[2, 4, 8],\n",
    "        mlp_ratio=4,\n",
    "        **kwargs\n",
    "    )\n",
    "    if pretrained:\n",
    "        state_dict = \\\n",
    "        torch.load('weights/pit_ti_distill_746.pth', map_location='cpu')\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "    \n",
    "class JigsawViT(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_cfg_file, num_labels=2):\n",
    "        super(JigsawViT, self).__init__()\n",
    "        # pretrained_cfg = timm.models.create_model(\n",
    "        #     'crossvit_base_240.in1k').default_cfg\n",
    "        # pretrained_cfg['file'] = pretrained_cfg_file\n",
    "        # self.model = timm.models.create_model(\n",
    "        #     'crossvit_base_240.in1k', pretrained=True, num_classes=num_labels, pretrained_cfg=pretrained_cfg)\n",
    "        pretrained_cfg = timm.models.create_model(\n",
    "            'pit_s_distilled_224.in1k').default_cfg\n",
    "        print(pretrained_cfg)\n",
    "        pretrained_cfg['file'] = pretrained_cfg_file\n",
    "        self.backbone = timm.models.create_model(\n",
    "            'pit_s_distilled_224.in1k', pretrained=True, features_only=True, pretrained_cfg=pretrained_cfg)\n",
    "        self.roi_align = get_roi_align_layer()\n",
    "        self.fc1 = nn.Linear(9216, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 512)\n",
    "        self.fc3 = nn.Linear(512, num_labels)\n",
    "        \n",
    "        \n",
    "    # @autocast()\n",
    "    def forward(self, x, roi):\n",
    "        features = self.backbone(x)[-1]\n",
    "        aligned_features = self.roi_align(features, rois)\n",
    "        x = aligned_features.view(aligned_features.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        output = self.fc3(x)\n",
    "        return output\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "model = pit_s(pretrained=False)\n",
    "# model.load_state_dict(torch.load('./weights/pit_s_809.pth'))\n",
    "print(model(torch.randn(1, 3, 224, 224)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e801e578-468d-4af8-9f41-05f4526fdded",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "vit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
